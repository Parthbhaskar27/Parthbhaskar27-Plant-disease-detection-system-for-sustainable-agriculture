
Summary of Improvisations in Your CNN Model
Increased Depth and Complexity

Added four convolutional layers with progressively increasing filters: 32 → 64 → 128 → 256.

This helps the model capture features at multiple levels of abstraction, improving its ability to distinguish complex patterns.

Use of Larger Kernel Sizes Initially

Used a 7×7 kernel in the first Conv layer and a 5×5 kernel in the second.

Larger kernels at early stages help capture broader features; smaller kernels (3×3) in deeper layers fine-tune details.

Multiple Pooling Layers

Strategically placed MaxPooling layers after some Conv layers to reduce spatial dimensions and computational load.

Two Fully Connected (Dense) Layers

Introduced Dense layers (128 and 64 units) before the output, enabling complex feature interaction before classification.

Dropout Regularization (partially implemented)

Added Dropout(0.5) layers after Dense layers (though not applied correctly initially).

Helps prevent overfitting by randomly disabling neurons during training.

Softmax Output for Multi-Class Classification

Correctly used Dense(38, activation='softmax') for 38-class classification.

